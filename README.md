# Simple but super PyTorch Linear layer CUDA extensen

You need to build before try Research\*.py

Go to:

```bash
myLinear_CUDA_backend
myKakuritsu_Linear_backend
```

To see the detailed information and instruction.

Research\*.py is to Debug and Research the Code! 

# Research

Go to [The Wiki](https://github.com/UEFI-code/MSRA_thePracticeSpaceProject_PyTorchCUDA/wiki) to see Research Status.

Quick Link: [My Project ReportðŸ˜Ž](https://github.com/UEFI-code/MSRA_thePracticeSpaceProject_PyTorchCUDA/wiki/Project-Report)

# Innovation

One Dim of CUDA blockID and threadID, Simple the MultiThread Idea.

Introduced the Kakuritsu Activation Method in Linear to Instead Dropout (Still need more ANN Experiment). 

# Good news!

You can try to build the CUDA code even you have NO GPU with my HACK!

My Cell Level Forward Available!

My Kaso Cell Backward Available!

CPU Inference Available!

Demo Code can run now!

Almost done, need a little understand of 'grad\_weights' and Experiment Report.

# Todo

## Add beautiful figures to explaining the theory.
## Add Omoshiroii CUDA operator Features.
## Finish the Experiment Report.
